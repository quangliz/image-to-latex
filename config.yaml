seed: 1234

trainer:
  accelerator: "gpu"
  devices: 1
  overfit_batches: 0.0
  check_val_every_n_epoch: 1
  fast_dev_run: false
  max_epochs: 15
  min_epochs: 1
  num_sanity_val_steps: 0
  enable_checkpointing: true
  log_every_n_steps: 100  # Log metrics every step
  enable_progress_bar: true
  enable_model_summary: true

callbacks:
  model_checkpoint:
    dirpath: "checkpoints"
    save_top_k: 2
    save_weights_only: true
    mode: "min"
    monitor: "val/loss_epoch"
    filename: "epoch_{epoch:02d}_valloss_{val/loss_epoch:.2f}"
  early_stopping:
    patience: 3
    mode: "min"
    monitor: "val/loss_epoch"
    min_delta: 0.001

data:
  batch_size: 8
  num_workers: 4
  pin_memory: true

lit_model:
  # Optimizer
  lr: 0.001
  weight_decay: 0.0001
  # Scheduler
  milestones: [10] 
  gamma: 0.5
  # Model
  d_model: 128     #64
  dim_feedforward: 256 #128
  nhead: 4
  dropout: 0.3
  num_decoder_layers: 2
  max_output_len: 150

logger:
  project: "image-to-latex"
  log_model: true
  offline: false  # Set to true if you want to log offline
  name: null  # Auto-generate a name
  save_dir: "wandb"  # Directory where wandb logs are saved
  version: null  # Auto-generate a version
  prefix: ""  # Prefix added to metric names
  job_type: "train"  # Type of job (train, test, etc.)

# Hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job_logging:
    root:
      level: INFO