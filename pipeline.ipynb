{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quangliz/Documents/schoolwork/py/CV/img2latex/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Built-in libraries\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import tarfile\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Third-party libraries\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import editdistance\n",
    "import hydra\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Metric\n",
    "from tqdm import tqdm\n",
    "import torchvision.models\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# PyTorch Lightning\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# Download files\n",
    "!wget -q -P data https://im2markup.yuntiandeng.com/data/im2latex_formulas.norm.lst\n",
    "!wget -q -P data https://im2markup.yuntiandeng.com/data/im2latex_validate_filter.lst\n",
    "!wget -q -P data https://im2markup.yuntiandeng.com/data/im2latex_train_filter.lst\n",
    "!wget -q -P data https://im2markup.yuntiandeng.com/data/im2latex_test_filter.lst\n",
    "\n",
    "# if you want to download the raw images, uncomment the following command and comment out the next command\n",
    "# !wget -q -P data https://im2markup.yuntiandeng.com/data/formula_images_processed.tar.gz\n",
    "# # Extract raw image data\n",
    "# !tar -xzf data/formula_images_processed.tar.gz -C data\n",
    "\n",
    "# this image data is processed \n",
    "!wget -q -P data https://im2markup.yuntiandeng.com/data/formula_images_processed.tar.gz\n",
    "# Extract processed image data\n",
    "!tar -xzf data/formula_images_processed.tar.gz -C data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/quangliz/Documents/schoolwork/py/CV/img2latex'),\n",
       " PosixPath('/home/quangliz/Documents/schoolwork/py/CV/img2latex/data'),\n",
       " PosixPath('/home/quangliz/Documents/schoolwork/py/CV/img2latex/data/vocab.json'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gốc dự án là thư mục hiện tại\n",
    "PROJECT_DIRNAME = Path().resolve()\n",
    "\n",
    "DATA_DIRNAME = PROJECT_DIRNAME / \"data\"\n",
    "\n",
    "RAW_IMAGES_DIRNAME = DATA_DIRNAME / \"formula_images\"\n",
    "PROCESSED_IMAGES_DIRNAME = DATA_DIRNAME / \"formula_images_processed\"\n",
    "VOCAB_FILE = DATA_DIRNAME / \"vocab.json\"\n",
    "PROJECT_DIRNAME, DATA_DIRNAME, VOCAB_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some utils\n",
    "def pil_loader(fp: Path, mode: str) -> Image.Image:\n",
    "    with open(fp, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(mode)\n",
    "    \n",
    "def get_all_formulas(filename: Path) -> List[List[str]]:\n",
    "    \"\"\"Returns all the formulas in the formula file.\"\"\"\n",
    "    with open(filename) as f:\n",
    "        all_formulas = [formula.strip(\"\\n\").split() for formula in f.readlines()]\n",
    "    return all_formulas\n",
    "\n",
    "def get_split(\n",
    "    all_formulas: List[List[str]],\n",
    "    filename: Path,\n",
    ") -> Tuple[List[str], List[List[str]]]:\n",
    "    image_names = []\n",
    "    formulas = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            img_name, formula_idx = line.strip(\"\\n\").split()\n",
    "            image_names.append(img_name)\n",
    "            formulas.append(all_formulas[int(formula_idx)])\n",
    "    return image_names, formulas\n",
    "\n",
    "\n",
    "# uncomment this if you want to preprocess the raw images\n",
    "# def first_and_last_nonzeros(arr):\n",
    "#     for i in range(len(arr)):\n",
    "#         if arr[i] != 0:\n",
    "#             break\n",
    "#     left = i\n",
    "#     for i in reversed(range(len(arr))):\n",
    "#         if arr[i] != 0:\n",
    "#             break\n",
    "#     right = i\n",
    "#     return left, right\n",
    "\n",
    "# def crop(filename: Path, padding: int = 8) -> Optional[Image.Image]:\n",
    "#     image = pil_loader(filename, mode=\"RGBA\")\n",
    "\n",
    "#     # Replace the transparency layer with a white background\n",
    "#     new_image = Image.new(\"RGBA\", image.size, \"WHITE\")\n",
    "#     new_image.paste(image, (0, 0), image)\n",
    "#     new_image = new_image.convert(\"L\")\n",
    "\n",
    "#     # Invert the color to have a black background and white text\n",
    "#     arr = 255 - np.array(new_image)\n",
    "\n",
    "#     # Area that has text should have nonzero pixel values\n",
    "#     row_sums = np.sum(arr, axis=1)\n",
    "#     col_sums = np.sum(arr, axis=0)\n",
    "#     y_start, y_end = first_and_last_nonzeros(row_sums)\n",
    "#     x_start, x_end = first_and_last_nonzeros(col_sums)\n",
    "\n",
    "#     # Some images have no text\n",
    "#     if y_start >= y_end or x_start >= x_end:\n",
    "#         print(f\"{filename.name} is ignored because it does not contain any text\")\n",
    "#         return None\n",
    "\n",
    "#     # Cropping\n",
    "#     cropped = arr[y_start : y_end + 1, x_start : x_end + 1]\n",
    "#     H, W = cropped.shape\n",
    "\n",
    "#     # Add paddings\n",
    "#     new_arr = np.zeros((H + padding * 2, W + padding * 2))\n",
    "#     new_arr[padding : H + padding, padding : W + padding] = cropped\n",
    "\n",
    "#     # Invert the color back to have a white background and black text\n",
    "#     new_arr = 255 - new_arr\n",
    "#     return Image.fromarray(new_arr).convert(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, token_to_index: Optional[Dict[str, int]] = None) -> None:\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.sos_token = \"<SOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "\n",
    "        self.token_to_index: Dict[str, int]\n",
    "        self.index_to_token: Dict[int, str]\n",
    "\n",
    "        if token_to_index:\n",
    "            self.token_to_index = token_to_index\n",
    "            self.index_to_token = {index: token for token, index in self.token_to_index.items()}\n",
    "            self.pad_index = self.token_to_index[self.pad_token]\n",
    "            self.sos_index = self.token_to_index[self.sos_token]\n",
    "            self.eos_index = self.token_to_index[self.eos_token]\n",
    "            self.unk_index = self.token_to_index[self.unk_token]\n",
    "        else:\n",
    "            self.token_to_index = {}\n",
    "            self.index_to_token = {}\n",
    "            self.pad_index = self._add_token(self.pad_token)\n",
    "            self.sos_index = self._add_token(self.sos_token)\n",
    "            self.eos_index = self._add_token(self.eos_token)\n",
    "            self.unk_index = self._add_token(self.unk_token)\n",
    "\n",
    "        self.ignore_indices = {self.pad_index, self.sos_index, self.eos_index, self.unk_index}\n",
    "\n",
    "    def _add_token(self, token: str) -> int:\n",
    "        \"\"\"Add one token to the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            token: The token to be added.\n",
    "\n",
    "        Returns:\n",
    "            The index of the input token.\n",
    "        \"\"\"\n",
    "        if token in self.token_to_index:\n",
    "            return self.token_to_index[token]\n",
    "        index = len(self)\n",
    "        self.token_to_index[token] = index\n",
    "        self.index_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def train(self, formulas: List[List[str]], min_count: int = 2) -> None:\n",
    "        \"\"\"Create a mapping from tokens to indices and vice versa.\n",
    "\n",
    "        Args:\n",
    "            formulas: Lists of tokens.\n",
    "            min_count: Tokens that appear fewer than `min_count` will not be\n",
    "                included in the mapping.\n",
    "        \"\"\"\n",
    "        # Count the frequency of each token\n",
    "        counter: Dict[str, int] = {}\n",
    "        for formula in formulas:\n",
    "            for token in formula:\n",
    "                counter[token] = counter.get(token, 0) + 1\n",
    "\n",
    "        for token, count in counter.items():\n",
    "            # Remove tokens that show up fewer than `min_count` times\n",
    "            if count < min_count:\n",
    "                continue\n",
    "            index = len(self)\n",
    "            self.index_to_token[index] = token\n",
    "            self.token_to_index[token] = index\n",
    "\n",
    "    def encode(self, formula: List[str]) -> List[int]:\n",
    "        indices = [self.sos_index]\n",
    "        for token in formula:\n",
    "            index = self.token_to_index.get(token, self.unk_index)\n",
    "            indices.append(index)\n",
    "        indices.append(self.eos_index)\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices: List[int], inference: bool = True) -> List[str]:\n",
    "        tokens = []\n",
    "        for index in indices:\n",
    "            if index not in self.index_to_token:\n",
    "                raise RuntimeError(f\"Found an unknown index {index}\")\n",
    "            if index == self.eos_index:\n",
    "                break\n",
    "            if inference and index in self.ignore_indices:\n",
    "                continue\n",
    "            token = self.index_to_token[index]\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "\n",
    "    def save(self, filename: Union[Path, str]):\n",
    "        \"\"\"Save token-to-index mapping to a json file.\"\"\"\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(self.token_to_index, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename: Union[Path, str]) -> \"Tokenizer\":\n",
    "        \"\"\"Create a `Tokenizer` from a mapping file outputted by `save`.\n",
    "\n",
    "        Args:\n",
    "            filename: Path to the file to read from.\n",
    "\n",
    "        Returns:\n",
    "            A `Tokenizer` object.\n",
    "        \"\"\"\n",
    "        with open(filename) as f:\n",
    "            token_to_index = json.load(f)\n",
    "        return cls(token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract regions of interest\n",
    "\n",
    "DATA_DIRNAME.mkdir(parents=True, exist_ok=True)\n",
    "cur_dir = os.getcwd()\n",
    "os.chdir(DATA_DIRNAME)\n",
    "\n",
    "if not PROCESSED_IMAGES_DIRNAME.exists():\n",
    "    PROCESSED_IMAGES_DIRNAME.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Cropping images...\")\n",
    "    for image_filename in RAW_IMAGES_DIRNAME.glob(\"*.png\"):\n",
    "        cropped_image = crop(image_filename, padding=8)\n",
    "        if not cropped_image:\n",
    "            continue\n",
    "        cropped_image.save(PROCESSED_IMAGES_DIRNAME / image_filename.name)\n",
    "\n",
    "# Clean the ground truth file\n",
    "cleaned_file = \"im2latex_formulas.norm.new.lst\"\n",
    "\n",
    "if not Path(cleaned_file).is_file():\n",
    "    print(\"Cleaning data...\")\n",
    "\n",
    "    with open(DATA_DIRNAME / \"im2latex_formulas.norm.lst\", \"r\", encoding=\"utf-8\") as infile, open(cleaned_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            line = re.sub(r'\\\\left\\(', '(', line)\n",
    "            line = re.sub(r'\\\\right\\)', ')', line)\n",
    "            line = re.sub(r'\\\\left\\[', '[', line)\n",
    "            line = re.sub(r'\\\\right\\]', ']', line)\n",
    "            line = re.sub(r'\\\\left\\{', '{', line)\n",
    "            line = re.sub(r'\\\\right\\}', '}', line)\n",
    "            line = re.sub(r'\\\\vspace\\s*\\{\\s*[^}]*\\s*\\}', '', line)\n",
    "            line = re.sub(r'\\\\hspace\\s*\\{\\s*[^}]*\\s*\\}', '', line)\n",
    "            outfile.write(line)\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "if not VOCAB_FILE.is_file():\n",
    "    print(\"Building vocabulary...\")\n",
    "    all_formulas = get_all_formulas(cleaned_file)\n",
    "    _, train_formulas = get_split(all_formulas, \"im2latex_train_filter.lst\")\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.train(train_formulas)\n",
    "    tokenizer.save(VOCAB_FILE)\n",
    "os.chdir(cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    \"\"\"A base Dataset class.\n",
    "\n",
    "    Args:\n",
    "        image_filenames: (N, *) feature vector.\n",
    "        targets: (N, *) target vector relative to data.\n",
    "        transform: Feature transformation.\n",
    "        target_transform: Target transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: Path,\n",
    "        image_filenames: List[str],\n",
    "        formulas: List[List[str]],\n",
    "        transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert len(image_filenames) == len(formulas)\n",
    "        self.root_dir = root_dir\n",
    "        self.image_filenames = image_filenames\n",
    "        self.formulas = formulas\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of samples.\"\"\"\n",
    "        return len(self.formulas)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Returns a sample from the dataset at the given index.\"\"\"\n",
    "        image_filename, formula = self.image_filenames[idx], self.formulas[idx]\n",
    "        image_filepath = self.root_dir / image_filename\n",
    "        if image_filepath.is_file():\n",
    "            image = pil_loader(image_filepath, mode=\"L\")\n",
    "        else:\n",
    "            # Returns a blank image if cannot find the image\n",
    "            image = Image.fromarray(np.full((64, 128), 255, dtype=np.uint8))\n",
    "            formula = []\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=np.array(image))[\"image\"]\n",
    "        return image, formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Im2Latex(LightningDataModule):\n",
    "    \"\"\"Data processing for the Im2Latex-100K dataset.\n",
    "\n",
    "    Args:\n",
    "        batch_size: The number of samples per batch.\n",
    "        num_workers: The number of subprocesses to use for data loading.\n",
    "        pin_memory: If True, the data loader will copy Tensors into CUDA pinned memory\n",
    "            before returning them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 8,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "        self.data_dirname = Path(__file__).resolve().parents[1] / \"data\"\n",
    "        self.vocab_file = Path(__file__).resolve().parents[1] / \"data\" / \"vocab.json\"\n",
    "        formula_file = self.data_dirname / \"im2latex_formulas.norm.new.lst\"\n",
    "        if not formula_file.is_file():\n",
    "            raise FileNotFoundError(\"Did you run scripts/prepare_data.py?\")\n",
    "        self.all_formulas = get_all_formulas(formula_file)\n",
    "        self.transform = {\n",
    "            \"train\": A.Compose(\n",
    "                [\n",
    "                    A.Affine(scale=(0.6, 1.0), rotate=(-1, 1), p=0.5),\n",
    "                    A.GaussNoise(p=0.5),\n",
    "                    A.GaussianBlur(blur_limit=(1, 1), p=0.5),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            ),\n",
    "            \"val/test\": ToTensorV2(),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def processed_images_dirname(self):\n",
    "        return self.data_dirname / \"formula_images_processed\"\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Load images and formulas, and assign them to a `torch Dataset`.\n",
    "\n",
    "        `self.train_dataset`, `self.val_dataset` and `self.test_dataset` will\n",
    "        be assigned after this method is called.\n",
    "        \"\"\"\n",
    "        self.tokenizer = Tokenizer.load(self.vocab_file)\n",
    "\n",
    "        if stage in (\"fit\", None):\n",
    "            train_image_names, train_formulas = get_split(\n",
    "                self.all_formulas,\n",
    "                self.data_dirname / \"im2latex_train_filter.lst\",\n",
    "            )\n",
    "            self.train_dataset = BaseDataset(\n",
    "                self.processed_images_dirname,\n",
    "                image_filenames=train_image_names,\n",
    "                formulas=train_formulas,\n",
    "                transform=self.transform[\"train\"],\n",
    "            )\n",
    "\n",
    "            val_image_names, val_formulas = get_split(\n",
    "                self.all_formulas,\n",
    "                self.data_dirname / \"im2latex_validate_filter.lst\",\n",
    "            )\n",
    "            self.val_dataset = BaseDataset(\n",
    "                self.processed_images_dirname,\n",
    "                image_filenames=val_image_names,\n",
    "                formulas=val_formulas,\n",
    "                transform=self.transform[\"val/test\"],\n",
    "            )\n",
    "\n",
    "        if stage in (\"test\", None):\n",
    "            test_image_names, test_formulas = get_split(\n",
    "                self.all_formulas,\n",
    "                self.data_dirname / \"im2latex_test_filter.lst\",\n",
    "            )\n",
    "            self.test_dataset = BaseDataset(\n",
    "                self.processed_images_dirname,\n",
    "                image_filenames=test_image_names,\n",
    "                formulas=test_formulas,\n",
    "                transform=self.transform[\"val/test\"],\n",
    "            )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        images, formulas = zip(*batch)\n",
    "        B = len(images)\n",
    "        max_H = max(image.shape[1] for image in images)\n",
    "        max_W = max(image.shape[2] for image in images)\n",
    "        max_length = max(len(formula) for formula in formulas)\n",
    "        padded_images = torch.zeros((B, 1, max_H, max_W))\n",
    "        batched_indices = torch.zeros((B, max_length + 2), dtype=torch.long)\n",
    "        for i in range(B):\n",
    "            H, W = images[i].shape[1], images[i].shape[2]\n",
    "            y, x = random.randint(0, max_H - H), random.randint(0, max_W - W)\n",
    "            padded_images[i, :, y : y + H, x : x + W] = images[i]\n",
    "            indices = self.tokenizer.encode(formulas[i])\n",
    "            batched_indices[i, : len(indices)] = torch.tensor(indices, dtype=torch.long)\n",
    "        return padded_images, batched_indices\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding2D(nn.Module):\n",
    "    \"\"\"2-D positional encodings for the feature maps produced by the encoder.\n",
    "\n",
    "    Following https://arxiv.org/abs/2103.06450 by Sumeet Singh.\n",
    "\n",
    "    Reference:\n",
    "    https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs/blob/main/lab9/text_recognizer/models/transformer_util.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_h: int = 2000, max_w: int = 2000) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        assert d_model % 2 == 0, f\"Embedding depth {d_model} is not even\"\n",
    "        pe = self.make_pe(d_model, max_h, max_w)  # (d_model, max_h, max_w)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_pe(d_model: int, max_h: int, max_w: int) -> Tensor:\n",
    "        \"\"\"Compute positional encoding.\"\"\"\n",
    "        pe_h = PositionalEncoding1D.make_pe(d_model=d_model // 2, max_len=max_h)  # (max_h, 1 d_model // 2)\n",
    "        pe_h = pe_h.permute(2, 0, 1).expand(-1, -1, max_w)  # (d_model // 2, max_h, max_w)\n",
    "\n",
    "        pe_w = PositionalEncoding1D.make_pe(d_model=d_model // 2, max_len=max_w)  # (max_w, 1, d_model // 2)\n",
    "        pe_w = pe_w.permute(2, 1, 0).expand(-1, max_h, -1)  # (d_model // 2, max_h, max_w)\n",
    "\n",
    "        pe = torch.cat([pe_h, pe_w], dim=0)  # (d_model, max_h, max_w)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: (B, d_model, H, W)\n",
    "\n",
    "        Returns:\n",
    "            (B, d_model, H, W)\n",
    "        \"\"\"\n",
    "        assert x.shape[1] == self.pe.shape[0]  # type: ignore\n",
    "        x = x + self.pe[:, : x.size(2), : x.size(3)]  # type: ignore\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    \"\"\"Classic Attention-is-all-you-need positional encoding.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = self.make_pe(d_model, max_len)  # (max_len, 1, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_pe(d_model: int, max_len: int) -> Tensor:\n",
    "        \"\"\"Compute positional encoding.\"\"\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: (S, B, d_model)\n",
    "\n",
    "        Returns:\n",
    "            (B, d_model, H, W)\n",
    "        \"\"\"\n",
    "        assert x.shape[2] == self.pe.shape[2]  # type: ignore\n",
    "        x = x + self.pe[: x.size(0)]  # type: ignore\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        dim_feedforward: int,\n",
    "        nhead: int,\n",
    "        dropout: float,\n",
    "        num_decoder_layers: int,\n",
    "        max_output_len: int,\n",
    "        sos_index: int,\n",
    "        eos_index: int,\n",
    "        pad_index: int,\n",
    "        num_classes: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_output_len = max_output_len + 2\n",
    "        self.sos_index = sos_index\n",
    "        self.eos_index = eos_index\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "        # Encoder\n",
    "        resnet = torchvision.models.resnet18(weights=None)\n",
    "        self.backbone = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "        )\n",
    "        self.bottleneck = nn.Conv2d(256, self.d_model, 1)\n",
    "        self.image_positional_encoder = PositionalEncoding2D(self.d_model)\n",
    "\n",
    "        # Decoder\n",
    "        self.embedding = nn.Embedding(num_classes, self.d_model)\n",
    "        self.y_mask = generate_square_subsequent_mask(self.max_output_len)\n",
    "        self.word_positional_encoder = PositionalEncoding1D(self.d_model, max_len=self.max_output_len)\n",
    "        transformer_decoder_layer = nn.TransformerDecoderLayer(self.d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(transformer_decoder_layer, num_decoder_layers)\n",
    "        self.fc = nn.Linear(self.d_model, num_classes)\n",
    "\n",
    "        # It is empirically important to initialize weights properly\n",
    "        if self.training:\n",
    "            self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        init_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "        nn.init.kaiming_normal_(\n",
    "            self.bottleneck.weight.data,\n",
    "            a=0,\n",
    "            mode=\"fan_out\",\n",
    "            nonlinearity=\"relu\",\n",
    "        )\n",
    "        if self.bottleneck.bias is not None:\n",
    "            _, fan_out = nn.init._calculate_fan_in_and_fan_out(self.bottleneck.weight.data)\n",
    "            bound = 1 / math.sqrt(fan_out)\n",
    "            nn.init.normal_(self.bottleneck.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: (B, _E, _H, _W)\n",
    "            y: (B, Sy) with elements in (0, num_classes - 1)\n",
    "\n",
    "        Returns:\n",
    "            (B, num_classes, Sy) logits\n",
    "        \"\"\"\n",
    "        encoded_x = self.encode(x)  # (Sx, B, E)\n",
    "        output = self.decode(y, encoded_x)  # (Sy, B, num_classes)\n",
    "        output = output.permute(1, 2, 0)  # (B, num_classes, Sy)\n",
    "        return output\n",
    "\n",
    "    def encode(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Encode inputs.\n",
    "\n",
    "        Args:\n",
    "            x: (B, C, _H, _W)\n",
    "\n",
    "        Returns:\n",
    "            (Sx, B, E)\n",
    "        \"\"\"\n",
    "        # Resnet expects 3 channels but training images are in gray scale\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        x = self.backbone(x)  # (B, RESNET_DIM, H, W); H = _H // 32, W = _W // 32\n",
    "        x = self.bottleneck(x)  # (B, E, H, W)\n",
    "        x = self.image_positional_encoder(x)  # (B, E, H, W)\n",
    "        x = x.flatten(start_dim=2)  # (B, E, H * W)\n",
    "        x = x.permute(2, 0, 1)  # (Sx, B, E); Sx = H * W\n",
    "        return x\n",
    "\n",
    "    def decode(self, y: Tensor, encoded_x: Tensor) -> Tensor:\n",
    "        \"\"\"Decode encoded inputs with teacher-forcing.\n",
    "\n",
    "        Args:\n",
    "            encoded_x: (Sx, B, E)\n",
    "            y: (B, Sy) with elements in (0, num_classes - 1)\n",
    "\n",
    "        Returns:\n",
    "            (Sy, B, num_classes) logits\n",
    "        \"\"\"\n",
    "        y = y.permute(1, 0)  # (Sy, B)\n",
    "        y = self.embedding(y) * math.sqrt(self.d_model)  # (Sy, B, E)\n",
    "        y = self.word_positional_encoder(y)  # (Sy, B, E)\n",
    "        Sy = y.shape[0]\n",
    "        y_mask = self.y_mask[:Sy, :Sy].type_as(encoded_x)  # (Sy, Sy)\n",
    "        output = self.transformer_decoder(y, encoded_x, y_mask)  # (Sy, B, E)\n",
    "        output = self.fc(output)  # (Sy, B, num_classes)\n",
    "        return output\n",
    "\n",
    "    def predict(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make predctions at inference time.\n",
    "\n",
    "        Args:\n",
    "            x: (B, C, H, W). Input images.\n",
    "\n",
    "        Returns:\n",
    "            (B, max_output_len) with elements in (0, num_classes - 1).\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        S = self.max_output_len\n",
    "\n",
    "        encoded_x = self.encode(x)  # (Sx, B, E)\n",
    "\n",
    "        output_indices = torch.full((B, S), self.pad_index).type_as(x).long()\n",
    "        output_indices[:, 0] = self.sos_index\n",
    "        has_ended = torch.full((B,), False)\n",
    "\n",
    "        for Sy in range(1, S):\n",
    "            y = output_indices[:, :Sy]  # (B, Sy)\n",
    "            logits = self.decode(y, encoded_x)  # (Sy, B, num_classes)\n",
    "            # Select the token with the highest conditional probability\n",
    "            output = torch.argmax(logits, dim=-1)  # (Sy, B)\n",
    "            output_indices[:, Sy] = output[-1:]  # Set the last output token\n",
    "\n",
    "            # Early stopping of prediction loop to speed up prediction\n",
    "            has_ended |= (output_indices[:, Sy] == self.eos_index).type_as(has_ended)\n",
    "            if torch.all(has_ended):\n",
    "                break\n",
    "\n",
    "        # Set all tokens after end token to be padding\n",
    "        eos_positions = find_first(output_indices, self.eos_index)\n",
    "        for i in range(B):\n",
    "            j = int(eos_positions[i].item()) + 1\n",
    "            output_indices[i, j:] = self.pad_index\n",
    "\n",
    "        return output_indices\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(size: int) -> Tensor:\n",
    "    \"\"\"Generate a triangular (size, size) mask.\"\"\"\n",
    "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def find_first(x: Tensor, element: Union[int, float], dim: int = 1) -> Tensor:\n",
    "    \"\"\"Find the first occurence of element in x along a given dimension.\n",
    "\n",
    "    Args:\n",
    "        x: The input tensor to be searched.\n",
    "        element: The number to look for.\n",
    "        dim: The dimension to reduce.\n",
    "\n",
    "    Returns:\n",
    "        Indices of the first occurence of the element in x. If not found, return the\n",
    "        length of x along dim.\n",
    "\n",
    "    Usage:\n",
    "        >>> first_element(Tensor([[1, 2, 3], [2, 3, 3], [1, 1, 1]]), 3)\n",
    "        tensor([2, 1, 3])\n",
    "\n",
    "    Reference:\n",
    "        https://discuss.pytorch.org/t/first-nonzero-index/24769/9\n",
    "\n",
    "        I fixed an edge case where the element we are looking for is at index 0. The\n",
    "        original algorithm will return the length of x instead of 0.\n",
    "    \"\"\"\n",
    "    mask = x == element\n",
    "    found, indices = ((mask.cumsum(dim) == 1) & mask).max(dim)\n",
    "    indices[(~found) & (indices == 0)] = x.shape[dim]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterErrorRate(Metric):\n",
    "    def __init__(self, ignore_indices: Set[int], *args):\n",
    "        super().__init__(*args)\n",
    "        self.ignore_indices = ignore_indices\n",
    "        self.add_state(\"error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.error: Tensor\n",
    "        self.total: Tensor\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        N = preds.shape[0]\n",
    "        for i in range(N):\n",
    "            pred = [token for token in preds[i].tolist() if token not in self.ignore_indices]\n",
    "            target = [token for token in targets[i].tolist() if token not in self.ignore_indices]\n",
    "            distance = editdistance.distance(pred, target)\n",
    "            if max(len(pred), len(target)) > 0:\n",
    "                self.error += distance / max(len(pred), len(target))\n",
    "        self.total += N\n",
    "\n",
    "    def compute(self) -> Tensor:\n",
    "        return self.error / self.total\n",
    "\n",
    "\n",
    "class ExactMatchScore(Metric):\n",
    "    def __init__(self, ignore_indices: Set[int], *args):\n",
    "        super().__init__(*args)\n",
    "        self.ignore_indices = ignore_indices\n",
    "        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.correct: Tensor\n",
    "        self.total: Tensor\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        N = preds.shape[0]\n",
    "        for i in range(N):\n",
    "            pred = [token for token in preds[i].tolist() if token not in self.ignore_indices]\n",
    "            target = [token for token in targets[i].tolist() if token not in self.ignore_indices]\n",
    "            if pred == target:\n",
    "                self.correct += 1\n",
    "        self.total += N\n",
    "\n",
    "    def compute(self) -> Tensor:\n",
    "        return self.correct / self.total\n",
    "\n",
    "\n",
    "class BLEUScore(Metric):\n",
    "    def __init__(self, ignore_indices: Set[int], *args):\n",
    "        super().__init__(*args)\n",
    "        self.ignore_indices = ignore_indices\n",
    "        self.add_state(\"score\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.score: Tensor\n",
    "        self.total: Tensor\n",
    "        self.smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        N = preds.shape[0]\n",
    "        for i in range(N):\n",
    "            pred = [token for token in preds[i].tolist() if token not in self.ignore_indices]\n",
    "            target = [token for token in targets[i].tolist() if token not in self.ignore_indices]\n",
    "\n",
    "            # Convert token IDs to strings for BLEU calculation\n",
    "            pred_str = [str(token) for token in pred]\n",
    "            target_str = [str(token) for token in target]\n",
    "\n",
    "            # Calculate BLEU score (using smoothing to handle edge cases)\n",
    "            if len(pred_str) > 0 and len(target_str) > 0:\n",
    "                bleu = sentence_bleu([target_str], pred_str, smoothing_function=self.smoothing_function)\n",
    "                self.score += torch.tensor(bleu)\n",
    "        self.total += N\n",
    "\n",
    "    def compute(self) -> Tensor:\n",
    "        return self.score / self.total\n",
    "\n",
    "\n",
    "class EditDistance(Metric):\n",
    "    def __init__(self, ignore_indices: Set[int], *args):\n",
    "        super().__init__(*args)\n",
    "        self.ignore_indices = ignore_indices\n",
    "        self.add_state(\"distance\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.distance: Tensor\n",
    "        self.total: Tensor\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        N = preds.shape[0]\n",
    "        for i in range(N):\n",
    "            pred = [token for token in preds[i].tolist() if token not in self.ignore_indices]\n",
    "            target = [token for token in targets[i].tolist() if token not in self.ignore_indices]\n",
    "\n",
    "            # Calculate raw edit distance\n",
    "            distance = editdistance.distance(pred, target)\n",
    "            self.distance += distance\n",
    "        self.total += N\n",
    "\n",
    "    def compute(self) -> Tensor:\n",
    "        return self.distance / self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitResNetTransformer(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        dim_feedforward: int,\n",
    "        nhead: int,\n",
    "        dropout: float,\n",
    "        num_decoder_layers: int,\n",
    "        max_output_len: int,\n",
    "        lr: float = 0.001,\n",
    "        weight_decay: float = 0.0001,\n",
    "        milestones: List[int] = [5],\n",
    "        gamma: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.milestones = milestones\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # For tracking epoch-level metrics\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "        vocab_file = Path(__file__).resolve().parents[1] / \"data\" / \"vocab.json\"\n",
    "        self.tokenizer = Tokenizer.load(vocab_file)\n",
    "        self.model = ResNetTransformer(\n",
    "            d_model=d_model,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            max_output_len=max_output_len,\n",
    "            sos_index=self.tokenizer.sos_index,\n",
    "            eos_index=self.tokenizer.eos_index,\n",
    "            pad_index=self.tokenizer.pad_index,\n",
    "            num_classes=len(self.tokenizer),\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_index)\n",
    "\n",
    "        # Training metrics\n",
    "        self.train_cer = CharacterErrorRate(self.tokenizer.ignore_indices)\n",
    "        self.train_exact_match = ExactMatchScore(self.tokenizer.ignore_indices)\n",
    "        self.train_bleu = BLEUScore(self.tokenizer.ignore_indices)\n",
    "        self.train_edit_distance = EditDistance(self.tokenizer.ignore_indices)\n",
    "\n",
    "        # Validation metrics\n",
    "        self.val_cer = CharacterErrorRate(self.tokenizer.ignore_indices)\n",
    "        self.val_exact_match = ExactMatchScore(self.tokenizer.ignore_indices)\n",
    "        self.val_bleu = BLEUScore(self.tokenizer.ignore_indices)\n",
    "        self.val_edit_distance = EditDistance(self.tokenizer.ignore_indices)\n",
    "\n",
    "        # Test metrics\n",
    "        self.test_cer = CharacterErrorRate(self.tokenizer.ignore_indices)\n",
    "        self.test_exact_match = ExactMatchScore(self.tokenizer.ignore_indices)\n",
    "        self.test_bleu = BLEUScore(self.tokenizer.ignore_indices)\n",
    "        self.test_edit_distance = EditDistance(self.tokenizer.ignore_indices)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, targets = batch\n",
    "        logits = self.model(imgs, targets[:, :-1])\n",
    "        loss = self.loss_fn(logits, targets[:, 1:])\n",
    "\n",
    "        # Log loss for each step (for progress bar and step-level tracking)\n",
    "        self.log(\"train/loss_step\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        # Store loss for epoch-level logging\n",
    "        self.training_step_outputs.append(loss.detach())\n",
    "\n",
    "        # Only calculate metrics on a small subset of batches (1%) to avoid slowing down training\n",
    "        # This gives us some metrics during training without significant slowdown\n",
    "        if batch_idx % 100 == 0:  # Calculate metrics every 100 batches\n",
    "            with torch.no_grad():  # Use no_grad to save memory\n",
    "                preds = self.model.predict(imgs)\n",
    "                self.train_cer.update(preds, targets)\n",
    "                self.train_exact_match.update(preds, targets)\n",
    "                self.train_bleu.update(preds, targets)\n",
    "                self.train_edit_distance.update(preds, targets)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # Calculate and log average training loss for the epoch\n",
    "        if len(self.training_step_outputs) > 0:\n",
    "            epoch_mean_loss = torch.stack(self.training_step_outputs).mean()\n",
    "            # Log the epoch average loss\n",
    "            self.log(\"train/loss_epoch\", epoch_mean_loss, prog_bar=True)\n",
    "            # Clear the list for the next epoch\n",
    "            self.training_step_outputs.clear()\n",
    "\n",
    "        # Calculate and log training metrics at the end of each epoch\n",
    "        if self.train_cer.total > 0:  # Only log if we have collected some data\n",
    "            self.log(\"train/cer\", self.train_cer.compute(), prog_bar=True)\n",
    "            self.log(\"train/exact_match\", self.train_exact_match.compute())\n",
    "            self.log(\"train/bleu\", self.train_bleu.compute())\n",
    "            self.log(\"train/edit_distance\", self.train_edit_distance.compute())\n",
    "\n",
    "            # Reset metrics for next epoch\n",
    "            self.train_cer.reset()\n",
    "            self.train_exact_match.reset()\n",
    "            self.train_bleu.reset()\n",
    "            self.train_edit_distance.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):  # batch_idx is required by PyTorch Lightning\n",
    "        imgs, targets = batch\n",
    "        logits = self.model(imgs, targets[:, :-1])\n",
    "        loss = self.loss_fn(logits, targets[:, 1:])\n",
    "\n",
    "        # Store loss for epoch-level logging\n",
    "        self.validation_step_outputs.append(loss.detach())\n",
    "\n",
    "        # Log step-level loss for debugging\n",
    "        self.log(\"val/loss_step\", loss, on_step=True, on_epoch=False, prog_bar=False)\n",
    "\n",
    "        with torch.no_grad():  # Use no_grad to save memory\n",
    "            preds = self.model.predict(imgs)\n",
    "            # Update metrics (don't log yet, will be logged at epoch end)\n",
    "            self.val_cer.update(preds, targets)\n",
    "            self.val_exact_match.update(preds, targets)\n",
    "            self.val_bleu.update(preds, targets)\n",
    "            self.val_edit_distance.update(preds, targets)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Calculate and log average validation loss for the epoch\n",
    "        if len(self.validation_step_outputs) > 0:\n",
    "            epoch_mean_loss = torch.stack(self.validation_step_outputs).mean()\n",
    "            # Log the epoch average loss with sync_dist=True\n",
    "            self.log(\"val/loss_epoch\", epoch_mean_loss, prog_bar=True, sync_dist=True)\n",
    "            # Clear the list for the next epoch\n",
    "            self.validation_step_outputs.clear()\n",
    "\n",
    "        # Log computed metrics\n",
    "        if self.val_cer.total > 0:\n",
    "            val_cer = self.val_cer.compute()\n",
    "            val_exact_match = self.val_exact_match.compute()\n",
    "            val_bleu = self.val_bleu.compute()\n",
    "            val_edit_distance = self.val_edit_distance.compute()\n",
    "\n",
    "            self.log(\"val/cer\", val_cer, prog_bar=True, sync_dist=True)\n",
    "            self.log(\"val/exact_match\", val_exact_match, sync_dist=True)\n",
    "            self.log(\"val/bleu\", val_bleu, sync_dist=True)\n",
    "            self.log(\"val/edit_distance\", val_edit_distance, sync_dist=True)\n",
    "\n",
    "            # Reset metrics for next epoch\n",
    "            self.val_cer.reset()\n",
    "            self.val_exact_match.reset()\n",
    "            self.val_bleu.reset()\n",
    "            self.val_edit_distance.reset()\n",
    "\n",
    "            # Print a message to confirm metrics were logged\n",
    "            print(f\"Validation metrics logged: loss_epoch={epoch_mean_loss:.4f}, cer={val_cer:.4f}, exact_match={val_exact_match:.4f}, bleu={val_bleu:.4f}, edit_distance={val_edit_distance:.4f}\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):  # batch_idx is required by PyTorch Lightning\n",
    "        imgs, targets = batch\n",
    "        with torch.no_grad():  # Use no_grad to save memory\n",
    "            preds = self.model.predict(imgs)\n",
    "            test_cer = self.test_cer(preds, targets)\n",
    "            test_exact_match = self.test_exact_match(preds, targets)\n",
    "            test_bleu = self.test_bleu(preds, targets)\n",
    "            test_edit_distance = self.test_edit_distance(preds, targets)\n",
    "\n",
    "            self.log(\"test/cer\", test_cer)\n",
    "            self.log(\"test/exact_match\", test_exact_match)\n",
    "            self.log(\"test/bleu\", test_bleu)\n",
    "            self.log(\"test/edit_distance\", test_edit_distance)\n",
    "            return preds\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        test_outputs = self.trainer.predict_loop.predictions\n",
    "        with open(\"test_predictions.txt\", \"w\") as f:\n",
    "            for preds in test_outputs:\n",
    "                for pred in preds:\n",
    "                    decoded = self.tokenizer.decode(pred.tolist())\n",
    "                    decoded.append(\"\\n\")\n",
    "                    decoded_str = \" \".join(decoded)\n",
    "                    f.write(decoded_str)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 1234,\n",
    "    \"trainer\": {\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"devices\": 1,\n",
    "        \"overfit_batches\": 0.0,\n",
    "        \"check_val_every_n_epoch\": 1,\n",
    "        \"fast_dev_run\": False,\n",
    "        \"max_epochs\": 15,\n",
    "        \"min_epochs\": 1,\n",
    "        \"num_sanity_val_steps\": 0,\n",
    "        \"enable_checkpointing\": True,\n",
    "        \"log_every_n_steps\": 100,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"enable_model_summary\": True,\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"model_checkpoint\": {\n",
    "            \"dirpath\": \"checkpoints\",\n",
    "            \"save_top_k\": 2,\n",
    "            \"save_weights_only\": True,\n",
    "            \"mode\": \"min\",\n",
    "            \"monitor\": \"val/loss_epoch\",\n",
    "            \"filename\": \"epoch_{epoch:02d}_valloss_{val/loss_epoch:.2f}\",\n",
    "        },\n",
    "        \"early_stopping\": {\n",
    "            \"patience\": 3,\n",
    "            \"mode\": \"min\",\n",
    "            \"monitor\": \"val/loss_epoch\",\n",
    "            \"min_delta\": 0.001,\n",
    "        },\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_memory\": True,\n",
    "    },\n",
    "    \"lit_model\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"milestones\": [10],\n",
    "        \"gamma\": 0.5,\n",
    "        \"d_model\": 64,\n",
    "        \"dim_feedforward\": 128,\n",
    "        \"nhead\": 2,\n",
    "        \"dropout\": 0.3,\n",
    "        \"num_decoder_layers\": 2,\n",
    "        \"max_output_len\": 150,\n",
    "    },\n",
    "    \"logger\": {\n",
    "        \"project\": \"image-to-latex\",\n",
    "        \"log_model\": True,\n",
    "        \"offline\": False,\n",
    "        \"name\": None,\n",
    "        \"save_dir\": \"wandb\",\n",
    "        \"version\": None,\n",
    "        \"prefix\": \"\",\n",
    "        \"job_type\": \"train\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "def dict_to_namespace(d):\n",
    "    ns = Namespace()\n",
    "    for k, v in d.items():\n",
    "        setattr(ns, k, dict_to_namespace(v) if isinstance(v, dict) else v)\n",
    "    return ns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Dataloader & model\n",
    "data_module = Im2Latex(**config[\"data\"])\n",
    "data_module.setup()\n",
    "\n",
    "lit_model = LitResNetTransformer(**config[\"lit_model\"])\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(**config[\"callbacks\"][\"model_checkpoint\"]),\n",
    "    EarlyStopping(**config[\"callbacks\"][\"early_stopping\"])\n",
    "]\n",
    "\n",
    "# Logger\n",
    "logger = WandbLogger(**config[\"logger\"])\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    **config[\"trainer\"],\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# Logging hyperparams\n",
    "if trainer.logger:\n",
    "    trainer.logger.log_hyperparams(dict_to_namespace(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "trainer.fit(lit_model, datamodule=data_module)\n",
    "trainer.test(lit_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inference ===\n",
    "\n",
    "# Determine device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====== Paths (nhập thủ công hoặc gán từ biến) ======\n",
    "image_path = \"\"\n",
    "checkpoint_path = \"\"\n",
    "\n",
    "# Check if paths exist\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    raise FileNotFoundError(f\"Checkpoint file not found at: {checkpoint_path}\")\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    raise FileNotFoundError(f\"Image file not found at: {image_path}\")\n",
    "\n",
    "# Load model\n",
    "model = LitResNetTransformer.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model.freeze()\n",
    "\n",
    "# Transform\n",
    "transform = ToTensorV2()\n",
    "\n",
    "# Load & preprocess image\n",
    "image = Image.open(image_path).convert(\"L\")\n",
    "image_tensor = transform(image=np.array(image))[\"image\"]\n",
    "image_tensor = image_tensor.unsqueeze(0).float().to(device)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    pred = model.model.predict(image_tensor)[0]\n",
    "    pred = pred.cpu() if device.type == \"cuda\" else pred\n",
    "    decoded = model.tokenizer.decode(pred.tolist())\n",
    "    decoded_str = \"\".join(decoded)\n",
    "\n",
    "print(\"=== LaTeX Prediction ===\")\n",
    "print(decoded_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
